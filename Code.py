# -*- coding: utf-8 -*-
"""GovEx Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g0hi2l3RS8LpxVk1CeZ8nsSwu443Ssh9
"""

# Please upload necessary csv files before running this notebook. (Sales.csv, improvements.csv, residential_building.csv, assessments.csv)

# from google.colab import drive
# drive.mount('/content/drive')

"""### Prompt and library initialization

The City of Syracuse approached us to build a predictive model of the house market sale prices
for all the properties in the city based on the physical characteristics of the building and other
metrics. They shared with us four different datasets (found here) :

● Sales

● Improvements

● Residential Building

● Assessments

The City also provided data dictionaries and some tips about the data:

● A parcel can contain several sites. Sometimes each site is a property, but other times it's
only an addition that will be sold together with the main house.

● Once an improvement is built (either at the same time of the construction or later), the
improvement is registered each time that there is an inspection of the property.

● Improvements are registered independently of the time of a sale. For the same property,
we might find improvements done both before and after a sale, but we only want to take
into account those that affect the sale price.

● There is a field in the sales table, ARMS_LENGHT (spelling corresponds with the original
city data), that provides information on whether or not the sale represents an
arms-length transaction. Therefore, it can be used to filter the sales considered not
representative.

● There are also sales that, despite being marked as arms_lenght transaction, might not
be. For example: Suspicious sales happen when a house is bought, refurbished, and sold
again within a small time frame, or when the sale price is very low.

We want to explore the data, and properly combine the datasets so that we have all of the
information in a unique table. The final table should have a unique sale in each row, with all the
building characteristics and improvements remarkable to that sale in the same row, and should
be ready for modeling (all values numeric). To prepare a final table:
1. Explore the data on each of the Sales, Improvements, and Residential Building tables.
2. Clean the data on the previous tables: look for duplications, missing or incorrect data,
data type issues, necessary filters, etc. If you had to make any assumption during that
process, it’s ok! Take notes and share them with us.
3. Combine the previous three datasets in a single table:

● Merge the datasets.

● Do any final cleaning with all the information combined, if necessary.

● Make sure each row in the final table is a unique sale (we want to predict sale
prices).

We also want to explore the assessments done by city inspectors and check whether there
might be any equity issues with the assessment process. Combining the assessment
information with the sale prices of properties, create a graph with sale prices on the x-axis and
the difference between assessment value (full market price) and sale price on the y-axis. Briefly
comment what you see with an equity angle.

**DATA DICTS**

**Assessments.CSV**<br>
**PARCEL_ID**	Numerical unique identifier for parcel<br>
**ROLL_YR**	Roll year<br>
**LAND_AV**	Land assessed value<br>
**TOTAL_AV**	Total assessed value<br>
**TIMESTAMP**	Unknown<br>
**FULL_MARKET_VALUE**	Assessed market value<br>

**Improvements.CSV**<br>
**PARCEL_ID**	Numerical unique identifier for parcel<br>
**SITE_NBR**	Numerical unique identifier for site<br>
**IMPROVE_NBR**	Improvement number<br>
**INV_DATE**	Sale date<br>
**STRUCTURE_CODE**	Code of the structure that affects the improvement<br>
**DIM1**	Dimension one<br>
**DIM2**	Dimension two<br>
**YR_BUILT**	Year built<br>
**GRADE**	Quality of the improvement<br>
**ROLL_YR**	Roll year<br>
**SALEPARCEL_IND**	S means there was a sale happening the year previous to the noted roll year<br>
**IMPROV_SQFT**	Square footage of the improvement<br>

**Residential Building.CSV**<br>
PARCEL_ID,"Section-Block-Lot number which includes the SWIS and the Check digit"<br>
SITE_NBR,"A site is defined as the land and/or buildings, which comprise a single unit for valuation. Each residence requires a separate site. If a parcel has several residences, they will have site numbers recorded as 01, 02, and so on"<br>
SALE_DATE,"Sale date"<br>
EXT_WALL_MATERIAL,"Material of the exterior wall (1: Wood, 2: Brick, 3: Aluminum/Vinyl, 4: Composition, 5: Concrete, 6: Stucco, 7: Stone, 8: Synthetic Material)"<br>
RBSMNT_TYP,"Basement type (1: Slab/pier, 2: Crawl, 3: Partial, 4: Full)"<br>
NBR_KITCHENS,"Number of complete kitchens with, at a minimum, a functional sink, range and/or oven, and a refrigerator"<br>
NBR_FULL_BATHS,"Number of full bathrooms, which consists of three or more fixtures; usually a watercloset, water basin, and bathtub and/or shower stall"<br>
NBR_BEDROOMS,"Number of rooms that were designed to be used primarily as a bedroom, even though they may currently be used as an office or den"<br>
NBR_FIREPLACES,"Number of openings for functional fireplaces. Woodstoves and freestanding fireplaces are not to be recorded here"<br>
CENTRAL_AIR,"Whether property has central heat (0,1)"<br>
BSMNT_GAR_CAP,"Basement garage capacity"<br>
OVERALL_COND,"Overall physical condition of the residence. Careful consideration should be given to interior walls and ceilings,   interior finish, kitchen cabinets and counters, heating, plumbing, and electrical equipment. Also considers exterior foundation,  chimneys, porches, siding, and roofing (1: Poor, 2: Fair, 3: Normal, 4: Good, 5: Excellent)"<br>
GRADE,"Overall construction grade. This item is used to record the overall construction grade of materials and quality of workmanship found in the residence (A: Excellent, B: Good, C: Average, D: Economy, E: Minimum)"<br>
GRADE_ADJUST_PCT,"Unknown"<br>
SFLA,"Total square footage of living area ncluding finished and unfinished areas"<br>
YR_BUILT,"Year of construction"<br>
NBR_STORIES,"Highest story height"<br>
BLDG_STYLE,"Classification of the residence as to architectural style (01: Ranch, 02: Raised Ranch, 03: Split Level, 04: Cape Cod, 05: Colonial, 06: Contemporary, 07: Mansion, 08: Old style, 09: Cottage, 10: Row, 11: Log Home, 12: Duplex, 13: Bungalow, 14: Other, 15: Town House, 16: A-Frame, 17: Manufactured Housing)"<br>
HEAT_TYPE,"Heat type (1: No central, 2: Hot air, 3: Hot water/steam, 4: Electric)"<br>
FUEL_TYPE,"Fuel type (1: None, 2: Natural Gas, 3: Electric, 4: Oil, 5: Wood, 6: Solar, 7: Coal, 8: Geothermal heat, 9: Propane/LPG)"<br>
TIMESTAMP,"Unknown"<br>
ROLL_YR,"Roll year"<br>
SALEPARCEL_IND,"S means there was a sale happening the year previous to the noted roll year"<br>
NBR_HALF_BATHS,"Number of full bathrooms, which consists of three or more fixtures; usually a water closet, water basin, and bathtub and/or shower stall"
FIRST_STORY,"First floor area in square feet"<br>
SECOND_STORY,"Second floor area in square feet"<br>
ADDL_STORY,"Square footage of all areas above the second floor which are not attic, half  story,  or  three  quarter story"<br>
HALF_STORY,"One-half of the floor area (measured from eave to eave) of the half story. To qualify as a half story, there must be at least 4 feet of exterior wall height at the eaves or sufficient slope of the roof for the area to be approximately 50 percent usable. It also should have windows of a size sufficient for light and ventilation"<br>
THREE_QTR_STORY,"Three quarters of the floor area measured from eave to eave of a three quarter story. Usually  there will be 5-7 feet of exterior wall height at the eaves giving a usable square footage that is  approximately 75 percent of the total floor area"<br>
FIN_OVER_GARAGE,"Finished area over garage: usable interior floor area over an attached garage in those residences where this area was designed to be part of the main living area, and is accessible from other parts of the main living area"<br>
FIN_ATTIC,"Finished attic area: usable interior floor area of finished attic, not to exceed 40 percent of the   total floor area. A finished attic would be characterized by a lack of headroom due to a low angle roof.    It will have finished walls, ceilings, and floors and adequate heat, lighting, and electricity"<br>
FIN_BASEMENT,"Finished basement area: square footage of basement area that has been finished with a quality of materials and workmanship consistent with the main living area"<br>
UNFIN_HALF_STORY,"One-half of the floor area (measured eave to eave) of unfinished half story"<br>
UNFIN_3_QTR_STORY,"Three quarters floor area (measured eave to eave) of unfinished half story"<br>
FIN_REC_ROOM,"Basement area that has finished living space with finished walls, floors, and ceilings, as well  as adequate lighting and heat, similar in quality to that found in the main living area, but not necessarily with the same materials"<br>
UNFIN_ROOM,"Square footage of full story area that has been left unfinished"
UNFIN_OVER_GARAGE,"Interior floor area over an attached garage in those residences where this area was designed to be part of  the main living area, but is unfinished"<br>
"PRINT_KEY","Parcel identification code"<br>


**Sales.CSV**<br>
PARCEL_ID, Numerical unique identifier for parcel<br>
SALE_DATE, Date of purchase<br>
SALETYPE, "Code for what real property was included in the sale of the property (1: Land only, 2: Building only, 3: Land and building, 4: Right of way or easement)"<br>
SALE_PRICE, "Actual amount of money, which was paid by the owner for the real property involved when the parcel  was purchased, rounded to the nearest one hundred dollars"<br>
VAL_USEABLE, Unknown<br>
NBR_PARCELS, Number of parcels<br>
MAP_PROVIDED, Unknown<br>
ARMS_LENGTH, "Whether or not the sale represents an arms-length transaction. "When value is 1 the transaction is arms-length, when is 0 it is not arms-length"<br>
TIMESTAMP, Unknown<br>
NET_SALE_PRICE,Net sale price<br>
ROLL_YR, Year<br>
OWNER_ID, Owner unique identifier<br>
ROLL_YR,Roll year<br>
PHYS_INSP_DATE, Physical inspection date<br>
PRINT_KEY,Parcel identification code<br>
"""

# 1 - Import necessary libraries
import pandas as pd
import os
import random as rd
import statsmodels as sm
import numpy as np
import matplotlib.pyplot as plt
from datetime import date
from datetime import datetime
import seaborn as sns

# Create data frames for each dataset
sales = pd.read_csv('/content/sales.csv')
improvements = pd.read_csv('/content/improvements.csv')
res_buildings = pd.read_csv('/content/residential_building.csv')
assessments = pd.read_csv('/content/assessments.csv')

"""### Initial Data Exploration"""

sales.info()
sales.head()
sales.shape
# Possible DATETYPES to use
# SALE_DATE
# ROLL_YR
# TIMESTAMP

improvements.info()
improvements.head()
improvements.shape
# Possible DATETYPES to use
# INV_DATE
# ROLL_YR

assessments = assessments.iloc[:,:6]
assessments.info()
assessments.head()
assessments.shape
# Possible DATETYPES to use
# ROLL_YR
# TIMESTAMP

res_buildings.info()
res_buildings.head()
res_buildings.shape
# Possible DATETYPES to use
# SALE_DATE
# ROLL_YR
# TIMESTAMP

"""### Sales Dataset Cleansing

- According to Homes.com and Realtor.com, the minimum price for any kind of property in Syracuse, NY is $1000. Any sales under this amount is considered as a suspicious sale. Thus, it is removed from the dataset. [Link](https://www.homes.com/syracuse-ny/?so=1&property_type=1,2,32,64,260,8), [Link 2](https://www.realtor.com/realestateandhomes-search/Syracuse_NY/sby-1).
- ROLL_YR column cannot have null value or 0 value. For that reason, those rows are removed from the sales table.
- ARMS_LENGTH values with 0 are removed in a part of the analysis to see the difference. The comparison with assessment table is done in both situations.
"""

# Display unique data types for each column
# We need one data type for the columns we will use for analysis to avoid missing values or duplications
for col in sales:
    print(col, sales.applymap(type)[col].unique())

# Converting all the PARCEL_ID values to integers. Remove the ones that we cannot convert.
# This conversion is done for all the datasets in this notebook.
undesired_parcel_indexes = []
print("Total length of column: " + str(len(sales['PARCEL_ID'])))
for i in range(len(sales['PARCEL_ID']) - 1):
  if isinstance(sales['PARCEL_ID'][i], str):
      try:
        int(sales['PARCEL_ID'][i])
      except Exception as e:
        print(e)
        print(sales['PARCEL_ID'][i])
        print("index:" + str(i))
        undesired_parcel_indexes.append(i)
  if isinstance(sales['PARCEL_ID'][i], float):
      print(sales['PARCEL_ID'][i])
      print("index:" + str(i))
      undesired_parcel_indexes.append(i)
print("Length of undesired indexes: " + str(len(undesired_parcel_indexes)))
print("Sales shape before filter: " + str(sales.shape))
sales.drop(index=undesired_parcel_indexes, axis=0,inplace=True)
print("Sales shape after filter: " + str(sales.shape))

# Display data type for PARCEL_ID
sales['PARCEL_ID'] = sales['PARCEL_ID'].astype('int64')
sales.applymap(type)['PARCEL_ID'].unique()

# Remove the sales which are smaller than $1000. They are considered as suspicious sales.
sales = sales[(sales['SALE_PRICE'] >= 1000)]
sales.info()
sales.shape

# If you consider sales with ARMS_LENGTH values 0 as suspicous sale, run this cell. Otherwise leave it commented out.
# Analysis for sale price vs full market value is done in both ways.

# print(sales['ARMS_LENGTH'].unique())
# print(sales.applymap(type)['ARMS_LENGTH'].unique())
# # Convert all the values to integers in ARMS_LENGTH table. Because the rows with the 0 values need to be dropped since they are considered as suspicious sale.
# sales['ARMS_LENGTH'] = sales['ARMS_LENGTH'].astype('int64')
# print(sales.applymap(type)['ARMS_LENGTH'].unique())
# sales = sales[(sales['ARMS_LENGTH'] == 1)] # drop 0 values from ARMS_LENGTH column because they are suspicious sales.
# print(sales.info())
# print(sales.shape)

# Drop nan and 0 values from ROLL_YR.
print(sales['ROLL_YR'].unique()) # DROP THE 0 AND nan VALUES FROM ROLL_YR COLUMN.
sales['ROLL_YR'] = sales['ROLL_YR'].replace(0, np.nan) # replace 0 values with nan
sales = sales.dropna(axis=0, subset=['ROLL_YR']) # drop NULL values from ROLL_YR column.
sales['ROLL_YR'] = sales['ROLL_YR'].astype('int64') # changing data type to integer for ROLL_YR column. Improvements table and res_buildings table have the same type for ROLL_YR column.
sales.info()
sales.shape

# Delete unneccesary columns from Sales table. If any column is 'unknown' per datadict, have dublication of the same column or have only value of 'nan' will be removed.
print(sales['PHYS_INSP_DATE'].unique()) # Display the unique 'PHYS_INSP_DATE' column

sales

# Delete unneccesary columns from Sales table. If any column is 'unknown' per datadict, have dublication of the same column or have only value of 'nan' will be removed.
sales = sales[['PARCEL_ID', 'SALE_DATE', 'SALETYPE', 'SALE_PRICE', 'NBR_PARCELS', 'ARMS_LENGTH', 'ROLL_YR', 'OWNER_ID', 'PRINT_KEY']]

# Display the sales dataset after cleaning
print(sales.info())
print(sales.describe())
print(sales.head())
print(sales.shape)

"""### Improvements Dataset Cleansing

- Improvements are registered independently of the time of a sale. For the same property, we might find improvements done both before and after a sale, but we only want to take into account those that affect the sale price.
"""

# Display improvements dataset
improvements.info()
improvements.head()

# Display unique data types for each column
# We need one data type for the columns we will use for analysis to avoid missing values or duplications
for col in improvements:
    print(col, improvements.applymap(type)[col].unique())

# Converting all the PARCEL_ID values to integers. Remove the ones that we cannot convert.
# This code block is not needed for Improvements table, since all the values are integers for the PARCEL_ID column.
imp_undesired_parcel_indexes = []
print("Total length of column: " + str(len(improvements['PARCEL_ID'])))
for i in range(len(improvements['PARCEL_ID']) - 1):
  if isinstance(improvements['PARCEL_ID'][i], str):
      try:
        int(improvements['PARCEL_ID'][i])
      except Exception as e:
        print(e)
        print(improvements['PARCEL_ID'][i])
        print("index:" + str(i))
        imp_undesired_parcel_indexes.append(i)
  if isinstance(improvements['PARCEL_ID'][i], float):
      print(improvements['PARCEL_ID'][i])
      print("index:" + str(i))
      imp_undesired_parcel_indexes.append(i)
print("Length of imp undesired indexes: " + str(len(imp_undesired_parcel_indexes)))
print("improvements shape before filter: " + str(improvements.shape))
improvements.drop(index=imp_undesired_parcel_indexes, axis=0,inplace=True)
print("improvements shape after filter: " + str(improvements.shape))
improvements['PARCEL_ID'] = improvements['PARCEL_ID'].astype('int64')

# INV_DATE refers to Sale date per data dictionary. We need one type of date which is the year of these values.
# Display the unique values in ROLL_YR and INV_DATE
print(improvements['ROLL_YR'].unique())
print(improvements['INV_DATE'].unique())
improvements['INV_DATE'] = pd.DatetimeIndex(improvements['INV_DATE']).year # changing inv_date to year
print(improvements.applymap(type)['INV_DATE'].unique())
print(improvements['INV_DATE'].unique())

improvements['STRUCTURE_CODE'] = improvements['STRUCTURE_CODE'].astype('str') # making data type to all strings in STRUCTURE_CODE column
# dropping ROLL_YR from improvements because it defeats:
# Improvements are registered independently of the time of a sale. For the same property, we might find improvements done both before and after a sale, but we only want to take into account those that affect the sale price.
# Since according to the original ROLL_YR, all improvements were done in 2019
improvements.drop(["ROLL_YR"], inplace=True, axis=1)
improvements.rename(columns={"INV_DATE": "ROLL_YR"}, inplace=True) # Changing INV_DATE to ROLL_YR to have the same column name with other datasets
improvements['ROLL_YR'] = improvements['ROLL_YR'].astype('int64')
improvements.info()
improvements.head()

"""### Residential Building Dataset Cleansing"""

# Display information about Residential Building dataset
res_buildings.info()

# Display Residential Buildings table
res_buildings

# Display the datatypes for each column in Residential Building dataset
for col in res_buildings:
    print(col, res_buildings.applymap(type)[col].unique())

res_buildings['GRADE'].unique()
res_buildings['GRADE'] = res_buildings['GRADE'].astype('str') # converting every value in GRADE column to strings
res_buildings.applymap(type)['GRADE'].unique()
res_buildings['PARCEL_ID'] = res_buildings['PARCEL_ID'].astype('int64') # converting every value in PARCEL_ID column to integers
res_buildings['ROLL_YR'] = res_buildings['ROLL_YR'].astype('int64')  # converting every value in ROLL_YR column to integers

"""### Assesments Dataset Cleaning"""

# Display unique data types for each column
# We need one data type for the columns we will use for analysis to avoid missing values or duplications
for col in assessments:
    print(col, assessments.applymap(type)[col].unique())

# Converting all the ROLL_YR values to integers. Remove the ones that we cannot convert.
assessments.info()
for col in assessments:
    print(col, assessments.applymap(type)[col].unique())
assessments.ROLL_YR.unique()
assessment_undesired_roll_yrs = []
print("Total length of column: " + str(len(assessments['ROLL_YR'])))
for i in range(len(assessments['ROLL_YR']) - 1):
  if isinstance(assessments['ROLL_YR'][i], str):
      try:
        int(assessments['ROLL_YR'][i])
      except Exception as e:
        print(e)
        print(assessments['ROLL_YR'][i])
        print("index:" + str(i))
        assessment_undesired_roll_yrs.append(i)
print("Length of undesired indexes: " + str(len(assessment_undesired_roll_yrs)))
print("assessments shape before filter: " + str(assessments.shape))
assessments.drop(index=assessment_undesired_roll_yrs, axis=0,inplace=True)
print("assessments shape after filter: " + str(assessments.shape))
assessments.ROLL_YR = assessments.ROLL_YR.astype('int64')

for col in assessments: # converting all the values in PARCEL_ID and ROLL_YR tp integers
    print(col, assessments.applymap(type)[col].unique())
assessments.ROLL_YR.unique()
assessments['PARCEL_ID'] = assessments['PARCEL_ID'].astype('int64')
assessments['ROLL_YR'] = assessments['ROLL_YR'].astype('int64')

for col in assessments: # converting all the values in PARCEL_ID and ROLL_YR to integers
    print(col, assessments.applymap(type)[col].unique())
assessments.ROLL_YR.unique()
assessments['PARCEL_ID'] = assessments['PARCEL_ID'].astype('int64')
assessments['ROLL_YR'] = assessments['ROLL_YR'].astype('int64')

print(assessments['ROLL_YR'].unique()) # years for assessments done
assessments.head()

"""### Analysis Answers

We want to explore the data, and properly combine the datasets so that we have all of the
information in a unique table. The final table should have a unique sale in each row, with all the
building characteristics and improvements remarkable to that sale in the same row, and should
be ready for modeling (all values numeric). To prepare a final table:
1. Explore the data on each of the Sales, Improvements, and Residential Building tables.
2. Clean the data on the previous tables: look for duplications, missing or incorrect data,
data type issues, necessary filters, etc. If you had to make any assumption during that
process, it’s ok! Take notes and share them with us.
3. Combine the previous three datasets in a single table:
● Merge the datasets.
● Do any final cleaning with all the information combined, if necessary.

● Make sure each row in the final table is a unique sale (we want to predict sale
prices).
"""

# Display each dataset's shape
print(sales.shape)
print(improvements.shape)
print(res_buildings.shape)
print(assessments.shape)

# merged_table = pd.merge(res_buildings, pd.merge(improvements, sales, on=['PARCEL_ID', 'ROLL_YR'], how='left'), on=['PARCEL_ID', 'ROLL_YR', 'SITE_NBR'], how='left')
# merged_table = merged_table[['ROLL_YR','PARCEL_ID', 'SITE_NBR', 'SALE_DATE_x', 'SALE_DATE_y', 'EXT_WALL_MATERIAL',
#        'RBSMNT_TYP', 'NBR_KITCHENS', 'NBR_FULL_BATHS', 'NBR_BEDROOMS',
#        'NBR_FIREPLACES', 'CENTRAL_AIR', 'BSMNT_GAR_CAP', 'OVERALL_COND',
#        'GRADE_x', 'GRADE_ADJUST_PCT', 'SFLA', 'YR_BUILT_x', 'NBR_STORIES',
#        'BLDG_STYLE', 'HEAT_TYPE', 'FUEL_TYPE', 'TIMESTAMP',
#        'SALEPARCEL_IND_x', 'NBR_HALF_BATHS', 'FIRST_STORY', 'SECOND_STORY',
#        'ADDL_STORY', 'HALF_STORY', 'THREE_QTR_STORY', 'FIN_OVER_GARAGE',
#        'FIN_ATTIC', 'FIN_BASEMENT', 'UNFIN_HALF_STORY', 'UNFIN_3_QTR_STORY',
#        'FIN_REC_ROOM', 'UNFIN_ROOM', 'UNFIN_OVER_GARAGE', 'PRINT_KEY_x',
#        'IMPROVE_NBR', 'INV_DATE', 'STRUCTURE_CODE', 'DIM1', 'DIM2',
#        'YR_BUILT_y', 'GRADE_y', 'SALEPARCEL_IND_y', 'IMPROV_SQFT', 'SALETYPE', 'SALE_PRICE', 'NBR_PARCELS', 'ARMS_LENGTH',
#        'OWNER_ID', 'PRINT_KEY_y']]

# Merging Sales, Residential Buildings, and Improvements datasets.
merged_table = pd.merge(sales, pd.merge(improvements, res_buildings, on=['PARCEL_ID', 'ROLL_YR', 'SITE_NBR'], how='left'), on=['PARCEL_ID', 'ROLL_YR'], how='left')
merged_table = merged_table[['ROLL_YR','PARCEL_ID', 'SITE_NBR','SALE_DATE_x', 'SALE_DATE_y', 'SALETYPE', 'SALE_PRICE', 'NBR_PARCELS',
       'ARMS_LENGTH', 'OWNER_ID', 'PRINT_KEY_x',
       'IMPROVE_NBR', 'STRUCTURE_CODE', 'DIM1', 'DIM2', 'YR_BUILT_x',
       'GRADE_x', 'SALEPARCEL_IND_x', 'IMPROV_SQFT',
       'EXT_WALL_MATERIAL', 'RBSMNT_TYP', 'NBR_KITCHENS', 'NBR_FULL_BATHS',
       'NBR_BEDROOMS', 'NBR_FIREPLACES', 'CENTRAL_AIR', 'BSMNT_GAR_CAP',
       'OVERALL_COND', 'GRADE_y', 'GRADE_ADJUST_PCT', 'SFLA', 'YR_BUILT_y',
       'NBR_STORIES', 'BLDG_STYLE', 'HEAT_TYPE', 'FUEL_TYPE', 'TIMESTAMP',
       'SALEPARCEL_IND_y', 'NBR_HALF_BATHS', 'FIRST_STORY', 'SECOND_STORY',
       'ADDL_STORY', 'HALF_STORY', 'THREE_QTR_STORY', 'FIN_OVER_GARAGE',
       'FIN_ATTIC', 'FIN_BASEMENT', 'UNFIN_HALF_STORY', 'UNFIN_3_QTR_STORY',
       'FIN_REC_ROOM', 'UNFIN_ROOM', 'UNFIN_OVER_GARAGE', 'PRINT_KEY_y']]
merged_table = merged_table[~merged_table['SITE_NBR'].isna()] # Drop nan values from merged table
merged_table[merged_table.duplicated(['ROLL_YR','PARCEL_ID', 'SITE_NBR'], keep='first')] # Drop duplicate values from merged table
merged_table

# merged_table = res_buildings.merge(improvements, on=['PARCEL_ID', 'ROLL_YR', 'SITE_NBR'], how='left').merge(sales, on=['PARCEL_ID', 'ROLL_YR'], how='left')
# merged_table = merged_table[['ROLL_YR','PARCEL_ID', 'SITE_NBR', 'SALE_DATE_x', 'SALE_DATE_y', 'EXT_WALL_MATERIAL',
#        'RBSMNT_TYP', 'NBR_KITCHENS', 'NBR_FULL_BATHS', 'NBR_BEDROOMS',
#        'NBR_FIREPLACES', 'CENTRAL_AIR', 'BSMNT_GAR_CAP', 'OVERALL_COND',
#        'GRADE_x', 'GRADE_ADJUST_PCT', 'SFLA', 'YR_BUILT_x', 'NBR_STORIES',
#        'BLDG_STYLE', 'HEAT_TYPE', 'FUEL_TYPE', 'TIMESTAMP',
#        'SALEPARCEL_IND_x', 'NBR_HALF_BATHS', 'FIRST_STORY', 'SECOND_STORY',
#        'ADDL_STORY', 'HALF_STORY', 'THREE_QTR_STORY', 'FIN_OVER_GARAGE',
#        'FIN_ATTIC', 'FIN_BASEMENT', 'UNFIN_HALF_STORY', 'UNFIN_3_QTR_STORY',
#        'FIN_REC_ROOM', 'UNFIN_ROOM', 'UNFIN_OVER_GARAGE', 'PRINT_KEY_x',
#        'IMPROVE_NBR', 'INV_DATE', 'STRUCTURE_CODE', 'DIM1', 'DIM2',
#        'YR_BUILT_y', 'GRADE_y', 'SALEPARCEL_IND_y', 'IMPROV_SQFT', 'SALETYPE', 'SALE_PRICE', 'NBR_PARCELS', 'ARMS_LENGTH',
#        'OWNER_ID', 'PRINT_KEY_y']]
merged_table = sales.merge(improvements, on=['PARCEL_ID' , 'ROLL_YR'], how='left').merge(res_buildings, on=['PARCEL_ID', 'ROLL_YR', 'SITE_NBR'], how='left')
merged_table = merged_table[['ROLL_YR','PARCEL_ID', 'SITE_NBR','SALE_DATE_x', 'SALE_DATE_y', 'SALETYPE', 'SALE_PRICE', 'NBR_PARCELS',
       'ARMS_LENGTH', 'OWNER_ID', 'PRINT_KEY_x',
       'IMPROVE_NBR', 'STRUCTURE_CODE', 'DIM1', 'DIM2', 'YR_BUILT_x',
       'GRADE_x', 'SALEPARCEL_IND_x', 'IMPROV_SQFT',
       'EXT_WALL_MATERIAL', 'RBSMNT_TYP', 'NBR_KITCHENS', 'NBR_FULL_BATHS',
       'NBR_BEDROOMS', 'NBR_FIREPLACES', 'CENTRAL_AIR', 'BSMNT_GAR_CAP',
       'OVERALL_COND', 'GRADE_y', 'GRADE_ADJUST_PCT', 'SFLA', 'YR_BUILT_y',
       'NBR_STORIES', 'BLDG_STYLE', 'HEAT_TYPE', 'FUEL_TYPE', 'TIMESTAMP',
       'SALEPARCEL_IND_y', 'NBR_HALF_BATHS', 'FIRST_STORY', 'SECOND_STORY',
       'ADDL_STORY', 'HALF_STORY', 'THREE_QTR_STORY', 'FIN_OVER_GARAGE',
       'FIN_ATTIC', 'FIN_BASEMENT', 'UNFIN_HALF_STORY', 'UNFIN_3_QTR_STORY',
       'FIN_REC_ROOM', 'UNFIN_ROOM', 'UNFIN_OVER_GARAGE', 'PRINT_KEY_y']]
merged_table = merged_table[~merged_table['SITE_NBR'].isna()] # Drop nan values from merged table
merged_table[merged_table.duplicated(['ROLL_YR','PARCEL_ID', 'SITE_NBR'], keep='first')] # Drop duplicate values from merged table
merged_table.info()
merged_table

"""#### Housing Market Sale Prices Prediction with Linear Regression Model


"""

# Determining features for price prediction models
prediction_merged_table = merged_table[['ROLL_YR','PARCEL_ID', 'SITE_NBR', 'SALE_PRICE', 'NBR_PARCELS',
       'OWNER_ID', 'IMPROVE_NBR', 'DIM1', 'DIM2', 'YR_BUILT_x',
       'IMPROV_SQFT', 'EXT_WALL_MATERIAL', 'RBSMNT_TYP', 'NBR_KITCHENS', 'NBR_FULL_BATHS',
       'NBR_BEDROOMS', 'NBR_FIREPLACES', 'CENTRAL_AIR', 'BSMNT_GAR_CAP',
       'OVERALL_COND', 'GRADE_ADJUST_PCT', 'SFLA', 'YR_BUILT_y',
       'NBR_STORIES', 'BLDG_STYLE', 'HEAT_TYPE', 'FUEL_TYPE', 'NBR_HALF_BATHS', 'FIRST_STORY', 'SECOND_STORY',
       'ADDL_STORY', 'HALF_STORY', 'THREE_QTR_STORY', 'FIN_OVER_GARAGE',
       'FIN_ATTIC', 'FIN_BASEMENT', 'UNFIN_HALF_STORY', 'UNFIN_3_QTR_STORY',
       'FIN_REC_ROOM', 'UNFIN_ROOM', 'UNFIN_OVER_GARAGE']]

prediction_merged_table = prediction_merged_table.fillna(0)
prediction_merged_table

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score

# Commented out IPython magic to ensure Python compatibility.
# Defining features
df_X = prediction_merged_table[['ROLL_YR','PARCEL_ID', 'SITE_NBR', 'NBR_PARCELS',
       'OWNER_ID', 'IMPROVE_NBR', 'DIM1', 'DIM2', 'YR_BUILT_x',
       'IMPROV_SQFT', 'EXT_WALL_MATERIAL', 'RBSMNT_TYP', 'NBR_KITCHENS', 'NBR_FULL_BATHS',
       'NBR_BEDROOMS', 'NBR_FIREPLACES', 'CENTRAL_AIR', 'BSMNT_GAR_CAP',
       'OVERALL_COND', 'GRADE_ADJUST_PCT', 'SFLA', 'YR_BUILT_y',
       'NBR_STORIES', 'BLDG_STYLE', 'HEAT_TYPE', 'FUEL_TYPE', 'NBR_HALF_BATHS', 'FIRST_STORY', 'SECOND_STORY',
       'ADDL_STORY', 'HALF_STORY', 'THREE_QTR_STORY', 'FIN_OVER_GARAGE',
       'FIN_ATTIC', 'FIN_BASEMENT', 'UNFIN_HALF_STORY', 'UNFIN_3_QTR_STORY',
       'FIN_REC_ROOM', 'UNFIN_ROOM', 'UNFIN_OVER_GARAGE']].to_numpy()

# Defining target variable
df_y = prediction_merged_table[['SALE_PRICE']].to_numpy()

# Split the data into training/testing sets
df_X_train = df_X[:-74]
df_X_test = df_X[-74:]

# Split the targets into training/testing sets
df_y_train = df_y[:-74]
df_y_test = df_y[-74:]

# Create linear regression object
regr = linear_model.LinearRegression()

# Train the model using the training sets
regr.fit(df_X_train, df_y_train)

# Make predictions using the testing set
df_y_pred = regr.predict(df_X_test)

# The coefficients
print('Coefficients: \n', regr.coef_)

# The mean squared error
print('Mean squared error: %.2f'
#      % mean_squared_error(df_y_test, df_y_pred))

# The coefficient of determination: 1 is perfect prediction
print('Coefficient of determination: %.2f'
#      % r2_score(df_y_test, df_y_pred))

# Printing predicted and actual values

for i in range(len(df_y_test)):
    print("Prediction:", df_y_pred[i], "vs. Actual:", df_y_test[i])

plt.plot(df_y_pred, label="Predicted values")
plt.title('Price Prediction with Linear Regression Model')
plt.plot(df_y_test, label="Actual values")
plt.ticklabel_format(style='plain', axis='y')
plt.ticklabel_format(style='plain', axis='x')
plt.xticks(rotation=90)
sns.set(rc={'figure.figsize':(20,8)})
plt.legend()

# Calculating R-squared score, mean absolute error, and mean squared error for linear regression
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

rms = mean_squared_error(df_y_test, df_y_pred, squared=False)

print("For LINEAR REGRESSION MODEL", '\n'
      "Mean absolute error is:", mean_absolute_error(df_y_test, df_y_pred), '\n',
      "Mean squared error is:", mean_squared_error(df_y_test, df_y_pred), '\n',
      "R-squared score is:", r2_score(df_y_test, df_y_pred), '\n',
      "Root mean squared error", rms)

"""#### Housing Market Sale Prices Prediction with Lasso Model"""

# Display SALE_PRICE as last column to fit in the models
prediction_merged_table = prediction_merged_table[['ROLL_YR','PARCEL_ID', 'SITE_NBR',  'NBR_PARCELS',
       'OWNER_ID', 'IMPROVE_NBR', 'DIM1', 'DIM2', 'YR_BUILT_x',
       'IMPROV_SQFT', 'EXT_WALL_MATERIAL', 'RBSMNT_TYP', 'NBR_KITCHENS', 'NBR_FULL_BATHS',
       'NBR_BEDROOMS', 'NBR_FIREPLACES', 'CENTRAL_AIR', 'BSMNT_GAR_CAP',
       'OVERALL_COND', 'GRADE_ADJUST_PCT', 'SFLA', 'YR_BUILT_y',
       'NBR_STORIES', 'BLDG_STYLE', 'HEAT_TYPE', 'FUEL_TYPE', 'NBR_HALF_BATHS', 'FIRST_STORY', 'SECOND_STORY',
       'ADDL_STORY', 'HALF_STORY', 'THREE_QTR_STORY', 'FIN_OVER_GARAGE',
       'FIN_ATTIC', 'FIN_BASEMENT', 'UNFIN_HALF_STORY', 'UNFIN_3_QTR_STORY',
       'FIN_REC_ROOM', 'UNFIN_ROOM', 'UNFIN_OVER_GARAGE','SALE_PRICE']]

# evaluate an lasso regression model on the dataset
from numpy import mean
from numpy import std
from numpy import absolute
from pandas import read_csv
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedKFold
from sklearn.linear_model import Lasso

data = prediction_merged_table.values
X, y = data[:, :-1], data[:, -1]
# define model
model = Lasso(alpha=1.0)
# define model evaluation method
cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)
# evaluate model
scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)
# force scores to be positive
scores = absolute(scores)
print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))

#define model
model = Lasso(alpha=1.0)

#fit model
model.fit(X, y)

# define new data
yhat = model.predict(df_X_test)
yhat

for i in range(len(df_y_test)):
    print("Prediction:", yhat[i], "vs. Actual:", df_y_test[i])

plt.plot(yhat, label="Predicted values")
plt.title('Price Prediction with Lasso Model')
plt.plot(df_y_test, label="Actual values")
plt.ticklabel_format(style='plain', axis='y')
plt.ticklabel_format(style='plain', axis='x')
plt.xticks(rotation=90)
plt.legend()

# Calculating R-squared score, mean absolute error, and mean squared error for Lasso Model
lasso_rms = mean_squared_error(df_y_test, yhat, squared=False)

print("For LASSO MODEL", '\n'
      "Mean absolute error is:", mean_absolute_error(df_y_test, yhat), '\n',
      "Mean squared error is:", mean_squared_error(df_y_test, yhat), '\n',
      "R-squared score is:", r2_score(df_y_test, yhat), '\n',
      "Root mean squared error", lasso_rms)

"""#### Housing Market Sale Prices Prediction with Random Forest Model"""

# evaluate random forest ensemble for regression
from numpy import mean
from numpy import std
from sklearn.datasets import make_regression
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedKFold
from sklearn.ensemble import RandomForestRegressor

data = prediction_merged_table.values
X, y = data[:, :-1], data[:, -1]
# define the model
model = RandomForestRegressor()
# evaluate the model
cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)
n_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')
scores = absolute(n_scores)
# report performance
print('MAE: %.3f (%.3f)' % (mean(scores), std(scores)))

# Making predictions
# fit the model on the whole dataset
model.fit(X, y)

# putting new row from test dataset
random_yhat = model.predict(df_X_test)
random_yhat

for i in range(len(df_y_test)):
    print("Prediction:", random_yhat[i], "vs. Actual:", df_y_test[i])

plt.plot(random_yhat, label="Predicted values")
plt.title('Price Prediction with Random Forest Model')
plt.plot(df_y_test, label="Actual values")
plt.ticklabel_format(style='plain', axis='y')
plt.ticklabel_format(style='plain', axis='x')
plt.xticks(rotation=90)
plt.legend()

# Calculating R-squared score, mean absolute error, and mean squared error for Random Forest Model
random_rms = mean_squared_error(df_y_test, random_yhat, squared=False)

print("For RANDOM FOREST MODEL", '\n'
      "Mean absolute error is:", mean_absolute_error(df_y_test, random_yhat), '\n',
      "Mean squared error is:", mean_squared_error(df_y_test, random_yhat), '\n',
      "R-squared score is:", r2_score(df_y_test, random_yhat), '\n',
      "Root mean squared error", random_rms)

"""### Graph Question

We also want to explore the assessments done by city inspectors and check whether there
might be any equity issues with the assessment process. Combining the assessment
information with the sale prices of properties, create a graph with sale prices on the x-axis and
the difference between assessment value (full market price) and sale price on the y-axis. Briefly
comment what you see with an equity angle.

"""

# Possible SALES DATETYPES to use
# SALE_DATE
# ROLL_YR
# TIMESTAMP
# Possible ASSESSMENTS DATETYPES to use
# ROLL_YR
# TIMESTAMP
# Possible MERGED TABLE DATETYPES to use
# SALE_DATE
# ROLL_YR
# TIMESTAMP

# Merge sales table w/ assessments table by PARCEL_ID
sales_w_assesments = pd.merge(sales, assessments, on=["PARCEL_ID"], how="left")
sales_w_assesments = sales_w_assesments[~sales_w_assesments['FULL_MARKET_VALUE'].isna()] # Drop nan values
sales_w_assesments['SALE_PRICE_MINUS_FULL_MARKET_VALUE'] = sales_w_assesments['SALE_PRICE']-sales_w_assesments['FULL_MARKET_VALUE'] # Calculate the difference between sale price and full market value
sales_w_assesments.SALE_PRICE.max()
sales_w_assesments.FULL_MARKET_VALUE.max()
sales_w_assesments = sales_w_assesments[sales_w_assesments['SALE_PRICE'] != sales_w_assesments['SALE_PRICE'].max()]
# sales_w_assesments.FULL_MARKET_VALUE.value_counts()
# sales_w_assesments.FULL_MARKET_VALUE.unique()

sns.scatterplot(x='SALE_PRICE', y='SALE_PRICE_MINUS_FULL_MARKET_VALUE', data=sales_w_assesments)
sns.set(rc={'figure.figsize':(20,8)})
plt.title('SALE PRICE V DIFF SALE PRICE MINUS FMV')
plt.ticklabel_format(style='plain', axis='y')
plt.ticklabel_format(style='plain', axis='x')
plt.xticks(rotation=90)
plt.ylim(-10000000, 28000000)
plt.xlim(0, 32000000)
plt.show()

# Merge assessments table with the merged table to display the price difference and find out if it is different from the analysis above
merged_table_w_assesments = pd.merge(merged_table, assessments, on=["PARCEL_ID"], how="left")
merged_table_w_assesments = merged_table_w_assesments[~merged_table_w_assesments['FULL_MARKET_VALUE'].isna()]
merged_table_w_assesments['SALE_PRICE_MINUS_FULL_MARKET_VALUE'] = merged_table_w_assesments['SALE_PRICE']-merged_table_w_assesments['FULL_MARKET_VALUE']
merged_table_w_assesments.SALE_PRICE.max()
# merged_table_w_assesments.FULL_MARKET_VALUE.max()
merged_table_w_assesments = merged_table_w_assesments[merged_table_w_assesments['SALE_PRICE'] != merged_table_w_assesments['SALE_PRICE'].max()]
# merged_table_w_assesments.FULL_MARKET_VALUE.value_counts()
# merged_table_w_assesments.FULL_MARKET_VALUE.unique()

sns.scatterplot(x='SALE_PRICE', y='SALE_PRICE_MINUS_FULL_MARKET_VALUE', data=merged_table_w_assesments)
sns.set(rc={'figure.figsize':(20,8)})
plt.title('SALE PRICE V DIFF SALE PRICE MINUS FMV')
plt.ticklabel_format(style='plain', axis='y')
plt.ticklabel_format(style='plain', axis='x')
plt.xticks(rotation=90)
plt.ylim(-1000000, 5000000)
plt.xlim(0, 17500000)
plt.show()

